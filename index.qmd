```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy=TRUE, tidy.opts=list(width.cutoff=50))
```

# MANAGHAL, DONNEES BIBLIOMETRIQUES, RESEAUX ET ANALYSES STATISTIQUES DE RESEAUX

```{r chargement des librairies, echo = FALSE, warning = FALSE, message=FALSE, tidy.opts=list(width.cutoff=50)}
library(igraph, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(managHAL, quietly = TRUE)
library(dplyr, quietly = TRUE)
library(reshape2, quietly = TRUE)
library (readxl, quietly = TRUE)
library(visNetwork, quietly = TRUE)
library(sbm, quietly = TRUE)

```

## Introduction

### Sciences Ouvertes

En 2017, AgroParisTech signe l’Appel de Jussieu pour promouvoir la bibliodiversité, visant à maintenir la diversité éditoriale et éviter la domination des grands groupes éditeurs. En partenariat avec l’INRAE, AgroParisTech, en partenariat avec INRAE, adopte en 2020 une politique de science ouverte ([\@science-ouverte](@science-ouverte), voir: La science ouverte à AgroParisTech – Politique d’établissement janvier 2020), facilitant la libre diffusion et la transparence des recherches scientifiques. Un exemple concret des principes appliqués suite à l'adoption de cette politique sont les principes FAIR pour la gestion des données de recherche : Findable (facilement trouvable), Accessible (accès clairement défini), Interoperable (intégrable avec d’autres outils), et Reusable (réutilisable avec toutes les informations nécessaires).

Pour atteindre ces objectifs, AgroParisTech crée le 2 janvier 2023 une cellule HAL, dirigée par la Direction de la Recherche, de l’Innovation et du Transfert Technologique, et par la Direction de la Documentation et du Patrimoine Culturel. Cette cellule facilite l'intégration de l'archive ouverte HAL dans la communauté de recherche d'AgroParisTech et veille à la qualité des métadonnées. HAL, l’archive nationale pour la diffusion ouverte des résultats de recherche, dispose d'un API permettant l’extraction et l’analyse des données via des outils comme Rstudio. Une explication plus détaillée de cet API et d'une requête HAL sera faite dans la section méthode.

### ManagHAL, package R d'extraction de données bibliométrique HAL et d'analyse de réseaux

Les outils pour réaliser des analyses bibliométriques se répartissent en trois catégories, "general bibliometric and performance analysis, science mapping analysis, and libraries" ([\@Moral-Muñoz2020](@Moral-Muñoz2020)) ou analyse générale bibliométrique et de performances, analyses par cartographie scientifique et librairies / packages. Le package managHAL se trouve dans les troisième et deusième catégories. Il existe à ce jour d'autres outils d'analyses bibliométriques mais la majorité ne permettent pas de travailler via l'api HAL et se concentrent sur webofscience, scopus, google scholar,... Il existe des outils d’extraction de publications HAL, cependant aucun package R ne permet actuellement d'extraire ces données pour des analyses statisistiques. Pour répondre à ce besoin, Théodore Vanrengterghem a commencé à développer le package R "ManagHAL". le package permet l’extraction de données HAL et la création de bilans bibliographiques. En cours de développement, ce package ne permet pas actuellement la réalisation d'analyses bibliométriques.

Ainsi, durant mes 2 mois et 17 jours de stage au sein de l’unité de recherche Mathématiques et Informatiques Appliquées (MIA) d’AgroParisTech Palaiseau, j’ai travaillé sur la généricité du package managHAL, ainsi que sur la programmation et l’intégration d’outils d’analyse statistique de réseau au sein du package ManagHAL. Encadré par deux tuteurs, Julie Albert et Pierre Barbillon, j’ai contribué à la mise en forme du package et à la création de fonctions spécifiques.

Pour comprendre les choix réalisés lors du développement du package, il est nécessaire de réaliser une introduction au contexte statistique des réseaux. Un réseau d’interaction est constitué de nœuds (les entités) et d’arêtes (les liens entre ces entités). Toute entité peut être modélisée comme un nœud, et leurs interactions comme des arêtes. Par exemple, les réseaux de gènes à partir de données de co-expression. Les réseaux se divisent en deux catégories principales : les réseaux Unipartites où tous les nœuds sont du même type et les réseaux Bipartites où il y a deux types distincts de nœuds, et les liens ne se forment qu’entre des nœuds de types différents.[@latent-block-models] Les interactions dans un réseau peuvent être binaires ou pondérées. Un réseau peut être représenté par une matrice d'adjacence $Y$ où :

```{=tex}
\begin{align*}
Y_{ij} \neq 0 & \quad \text{si un lien existe entre les nœuds } i \text{ et } j, \\
Y_{ij} = 0 & \quad \text{si aucun lien n'existe entre les nœuds } i \text{ et } j.
\end{align*}
```
Nous décrivons les noeuds d'un réseau comme : $V = { 1,..,N }$ , ($N$ correspond à la taille du réseau). Les arrêtes peuvent être décrites comme $E = \{ (i,j) | i,j \in V \}$ représentant les paires de nœuds entre lesquels il existe une interaction. Chaque arête $(i,j)$ indique une connexion entre les nœuds $i$ et $j$.

Il existe plusieurs approches différentes pour analyser la structure d’un réseau. Dans ce mémoire, une seule approche sera considérée. Elle suppose l’existence de groupe fonctionnels partageant un même patron de connexion. Nous nous baserons par la suite sur une méthode possible pour cette approche: les modèles probabilistes génératifs. Cette approche sera détaillée dans la section méthode.

Ce package s'addresse donc à un public souhaitant réaliser des analyses bibliométriques approfondis à partir de HAL. Nous verrons cependant dans la section discussion qu'il est possible d'élargir les usages du packages.

## METHODES

### sbm

Cette partie est principalement basée sur la publication "Using Latent Block Models to Detect Structure in Ecological Networks" @latent-block-models

Les Modèles en Bloc Stochastiques (aussi appelé SBM) sont des modèles probabilistes. Ils supposent que les nœuds d’un même réseau sont divisés en blocs (groupe, clusters) latents regroupant les entités ayant des modèles de connectivité similaires. Latents signifie qu’il n’y a pas d’apriori sur le type de structure recherchée. Ces modèles sont utiles pour détecter les communautés, les clusters aux sein d'un réseau complexe. Pour réaliser un modèle en bloc stochastique, il est nécessaire de supposer que le réseau obtenu à partir de mes données est une réalisation de mon modèle.

Les noeuds sont partitionnés en $K$ groupes latents. On définit un vecteur $Z$ où $Zi = k$ , $k$ est le bloc auquel appartient le noeud $i$ . Dans le cas d'un réseau unipartite, on définit :

-   $P(Zi = k) = \pi_k$ , comme la probabilité d'appartenance au groupe $k$.

-   $P(Y_{ij} = 1 | Z_i = k, Z_j = k') = \gamma_{kk'}$ , comme la probabilité pour une paire de noeuds appartenant à un bloc $k$ et à un bloc $k'$ d'être en interaction (dans le cas binaire).

Nous avons donc comme paramètres du modèle $\theta = (\gamma, \pi)$. A partir du réseau connu, il est possible d'inférer les paramètres du modèle. Par nécessité d'écourter le rapport, une explication détaillée ne sera pas réalisée. Je vous conseille cependant d'aller voir la publication "Using Latent Block Models to Detect Structure in Ecological Networks" @latent-block-models où les explications à ce propos sont claires et précises.

### méthodes informatiques

Comme précisé dans l'introduction, l'entièreté des méthodes informatiques développées ont été réalisées sur RStudio en utilisant le langage R version 4.4.1 sous windows 11 x64. Les packages utilisés sont les suivants :

Le package dplyr [@dplyr] est utilisé pour la manipulation et la transformation des données grâce à une syntaxe claire et cohérente. Ce package facilite les opérations courantes telles que la sélection, le filtrage, le regroupement et le résumé des données. Le package mailR [@mailR] est utilisé pour l'envoi d'emails avec des pièces jointes directement depuis R, ce qui est utile pour automatiser l'envoi de rapports. Le package askpass [@askpass] est utilisé pour gérer les mots de passe de manière sécurisée. Le package furrr [@furrr] est utilisé pour paralléliser les tâches en combinaison avec le package future [@future]. Le package future [@future] permet l'exécution de code en parallèle, ce qui réduit le temps de traitement des tâches lourdes. Le package ggplot2 [@ggplot2] est utilisé pour la création de visualisations graphiques sophistiquées. Le package matgrittr [@magrittr] fournit des opérateurs pour améliorer la lisibilité du code, en particulier l'opérateur `%>%` (pipe). Le package purrr [@purrr] est utilisé pour les opérations fonctionnelles sur les listes et autres structures de données. Le package quarto [@quarto] est utilisé pour la création de documents dynamiques et de rapports. Le package readxl [@readxl] permet la lecture de fichiers Excel. Le package rvest [@rvest] est utilisé pour le web scraping, c'est-à-dire l'extraction de données à partir de pages web. Le package SnowballC [@SnowballC] est utilisé pour le traitement de la langue naturelle, en particulier pour la lemmatisation. Le package stringr [@stringr] fournit des fonctions pour la manipulation des chaînes de caractères. Le package tictoc [@tictoc] est utilisé pour mesurer le temps d'exécution des morceaux de code. Le package tm [@tm] est utilisé pour le text mining (extraction de connaissances à partir de textes). Le package wordcloud [@wordcloud] est utilisé pour la création de nuages de mots. Le package Visnetwork **\[la citation ne fonctionne pas\]** est utilisé pour la visualisation de réseaux dynamiques. Le package Sbm [@sbm] est utilisé pour réaliser les analyses statisitiques de réseaux.

Afin de pouvoir construire une requête HAL pour récupèrer des données bibliographiques de publications d'auteurs, il est nécessaire de comprendre comment construire une requête. Pour construire une requête HAL, il faut au moins un paramètre `q` qui contient la requête de recherche. Ce paramètre doit spécifier le champ de recherche suivi de la valeur. Par exemple, pour rechercher le terme "test" : `http://api.archives-ouvertes.fr/search/?q=test&wt=xml` . Si le champ n'est pas spécifié, la recherche s'effectue par défaut dans l'index texte. Pour rechercher dans un champ particulier, il faut utiliser `champ:terme` : `http://api.archives-ouvertes.fr/search/?q=title_t:japon&wt=xml` . Pour échapper certains caractères spéciaux utilisés par Apache Solr, utilisez `\`. Par exemple, `(1+1):2` devient `\(1\+1\)\:2`. (voir @api-hal). Il est possible d'indiquer le format de la réponse (sous-entendant le format des données récupérées) via le paramètre `wt` . Pour finir il est possible de spécifier les champs à retourner dans la réponse via le paramètre `fl` et de faire un filtre des données via le paramètre `fq` . Tous les possibilités de l'API HAL n'ont pas été abordé dans ce court résumé. Il est pourtant compréhensible que l'API HAL est très diverse et permet une grande liberté à l'utilisateur pour l'extraction des données. De plus, il existe un nombre de champs considérables. Certains seront abordés par la suite dans la section résultats.

Pour finir, l'entièreté du stage s'est inscrit dans une dynamique de développement et d'intégration continue via la forge gitlab d'AgroParisTech. La forge gitLab MIA, du partenariat INRAE et AgroParisTech, est un logiciel DevOps passant par le language Git. Git est un système de controle, de versionnage du code des dévéloppeurs gratuit et open source. Il permet à plusieurs développeurs de travailler sur un code sur un même dépot et d'avancer ensemble par le biais de "merge". "Merge" siginife que la personne modifiant un fichier contenant du code pousse en ligne sur le depot partagé , les modifications réalisées.

## RESULTATS

Cette section présente les résultats obtenus au cours de mon stage. Elle est divisée en deux parties principales: les fonctions et algorithmes développés, modifiés ainsi que les résultats obtenus via un exemple. Cet exemple consistera en des données provenant des Ressources Humaines du Laboratoire MIA d'AgroPariTech. Chaque fonction est expliquée en détail, suivie des raisons justifiant son développement. Il est important de noter que mon stage se terminant le 26 juillet, Le package est donc encore en développement. Les fonctions présentées fonctionnent dans le cadre de l'exemple. Dans cette section, les noms de *`fonctions`* seront en italique et les noms des **scripts R** contenant les fonctions seront en gras.

### Accessibilité et généricité de ManagHAL

Lors de mon stage, j'ai d'abord décidé de rendre le package plus accessible et générique. Pour ce faire, il m'était nécessaire de réorganiser et modifier certaines fonctions déjà présentes codées par Théodore Vanrengterghem.

#### Modifications de la fonction *`load_mia_table`*

J'ai commencé par modifier dans le fichier **mia_table.R** la fonction *`load_mia_table`*. A l'origine, cette fonction permettait le chargement d'un csv en ligne depuis une addresse spécifique vers un fichier contenant des informations tel que le nom, le prenom, l'idhal, l'equipe, etc, des personnes appartenant au labo. J'ai fait le choix de renommer le fichier **load_table.R**. J'ai remplacé la fonction *`load_mia_table`* par trois fonctions *`load_team_table_csv`*, *`load_team_table_url`*, et *`load_team_table`*. *`load_team_table_csv`* permet de charger un csv dans Rstudio depuis un fichier local de la machine de l'utilisateur. *`load_team_table_url`* permet de charger un csv dans Rstudio depuis un fichier en ligne. *load_team_table* fait appel aux deux fonctions précédentes et permet par le biais d'argument de fonctions de spécifier la provenance du fichier et de charger le csv soit depuis un url soit depuis un fichier local. J'ai aussi modifier la majorité des fonctions déjà présentes dans le package faisant appel à *load_mia_table*. Les paramètres en entrée des trois fonctions sont les même que la fonction *`load_mia_table`* d'origine avec l'ajout d'un paramètre permettant à l'utilisateur de rentrer l'adresse du fichier. J'ai ensuite crée une documentation via le package "Roxygène2" conforme aux critères requis pour un package mis en ligne (veuillez vous referez à l'annexe : [Exemple d'une documentation classique avec Roxygen2](#doc-Roxygen2) ) pour chacune des trois fonctions.

La fonction originelle *`load_mia_table`* ne permettait que de charger de manière spécifique le csv fourni par les Ressources Humaines depuis un url seafile. Cette fonction n'était pas générique. Ainsi, il était nécéssaire de la modifier. J'ai décidé de diviser en trois fonctions différentes afin de permettre à l'utilisateur de choisir de charger des tables de provenance locale ou depuis internet. J'ai décidé pour *`load_team_table_url`* de garder en majeur partie le code d'origine et de simplement le réadapter par soucis de temps. J'y ai ajouté des conditions de présence de colonnes obligatoires afin d'obliger l'utilisateur à fournir des colonnes sous un format spécifique. (Pour voir le corps de la fonction, veuillez vous référer à l'annexe : [load_team_table_url](#load-team-table-url))

*`load_team_table_csv`* est basé sur la construction de *`load_team_table_url`*. Je l'ai adapté pour pouvoir charger depuis un fichier local en utilisant *read.csv*. De même que dans *`load_team_table_url`*, j'y ai ajouté des conditions de présence de colonnes obligatoires afin d'obliger l'utilisateur à fournir des colonnes sous un format spécifique. (Pour voir le corps de la fonction, veuillez vous référer à l'annexe : [load_team_table_csv](#load-team-table-csv))

Enfin, *`load_team_table`* appelle l'une des deux fonctions en fonction du type d'adresse fourni : URL ou chemin de fichier local. Le type de l'adresse est déterminé en utilisant *`grepl`*, qui utilise une expression régulière. (Pour voir le corps de la fonction, veuillez vous référer à l'annexe : [load_team_table](#load-team-table))

#### Résultat de l'Application de *load_team_table*

```{r run example load_team_table, warning=FALSE, tidy.opts=list(width.cutoff=50)}
data_RH_csv_Example <- load_team_table(filter_id = TRUE, date_cols = c(6,7), filepath_or_url = "./data/Data_RH_Example.csv")
# afin de ne garder que les lignes avec un idhal correct
data_RH_csv_Example <- data_RH_csv_Example[which(!is.na(data_RH_csv_Example$idhal)),]
head(data_RH_csv_Example)
```

Lors de l'application de la fonction *`load_team_table`* ci-dessus en utilisant le jeu de données test Data_RH_Example, on observe le chargement d'un csv contenant les colonnes d'origines du csv fournie en entrée. Les valeurs manquantes ou mal renseignées des idhals numériques ont été remplacées par des NAs. Les dates présentes deans les colonnes de dates spécifiés par l'utilisateur ont été nettoyées et ont le bon format.

#### Modifications des fonctions régissant la requête HAL

Dans le package managHAL originel, il y avait deux fichiers contenant les fonctions nécéssaires à la construction d'une requête HAL. Ces deux fichiers se nomment **HAL_reports.R** et **HAL_queries.R**. La fonction principale de construction d'une requête HAL est *`HAL_query`*. Elle prend en entrée différents paramètres et construit une url correspondant à une requête HAL. Ces différents paramètres définissent les différents champs, filtres, sorties demandées de la requête. Les champs de sortie ne convenant pas et manquait d'information pour la construction d'un réseau. Ainsi, après réflexion et discussion avec mes tuteurs Julie Aubert et Pierre Barbillon, j'ai modifié les différents champs de sorties afin d'obtenir les données nécessaires et essentielles à la construction d'un réseau. Par exemple, le champ 'authFullNamePersonIDIDHal_fs' permet de spécifier à HAL_query de récupérer les noms et les identifiants HAL numériques des autheurs d'une même publication. Le paramètre add_output ajouté par Théodore Vanrengterghem dans la fonction d'origine permet à l'utilisateur de rentrer des champs de sortie spécifiques en plus. Ce paramètre a été beaucoup utilisé au cours de ce stage dans la création de fonction permettant la création de réseau.

La fonction d'origine ne créer pas d'url d'une page permettant de récupérer les publications autres qu'à partir d'identifiants numériques d'auteurs, j'ai modifié les paramètres d'entrée et le corps de la fonction afin que l'utilisateur puissent spécifier si les identifiants fournis sont des identifiants de structures ou des identifiants de personnes. La nouvelle fonction HAL_query créer donc un url qui peut donc récupérer au choix de l'utilisateur une liste de publications associés à une structure via son identifiant ou une liste de publications associés à un identifiant numérique. (Pour voir le corps de la fonction, veuillez vous référer à l'annexe : [HAL_query](#HAL-query))

Plusieurs fonctions présentes avant mes modifications faisait appel à HAL_query. Ces dernières comme HAL_extract_csv ont donc été modifiées afin de corespondre à la nouvelle version de HAL_query. De même Les fonctions auquelles fait appel HAL_query ont elles aussi été modifiées pour correspondre à HAL_query. HAL_extract_csv est une fonction récupèrant l'url via HAL_query. Pour montrer les résultats obtenus avec HAL_query, des exemples ci-dessous seront réalisés avec HAL_extract_csv pour une liste d'identifiants numérique d'auteurs et pour un identifiant d'une unité/laboratoire. Il sera ajouté des sorties via le paramètres add_outputs :

```{r exemple utiliation HAL extract csv, tidy.opts=list(width.cutoff=50)}
date_min = "01/01/2022"
date_max = "01/01/2023"

# Publications à partir d'un id de laboratoire.
HAL_publis_Labo <- managHAL::HAL_extract_csv(id = 1002311, date_min,date_max, type_id = "struct_id", add_output = c("structAcronym_s","structId_i", "structHasAlphaAuthIdHalPersonid_fs"))
HAL_publis_Labo[1,]

# Publications à partir d'ids auteurs
HAL_publis_auteurs <- managHAL::HAL_extract_csv(id = data_RH_csv_Example[,12] , date_min,date_max, type_id = "person_id")
HAL_publis_auteurs[1,]
```

### Ajout de fonctionnalité à ManagHAL

#### Création d'un réseau de co-auteurs avec Visnetwork

Afin d'obtenir les informations sous-jacentes des informations bibliographiques, j'ai commencé à travailler à la création d'un réseau de co-auteur. Pour l'instant, la fonctionalité n'existe qu'au format de script R sur un document quarto. Au cours des semaines restantes, l'objectif est de continuer à travailler dessus et créer des fonctions simples et claires permettant à l'utilisateur de créer son propre réseau à partir de ses données. Le script de création de fonction est inspiré d'un script fournie au préalable par ma tutrice Julie Aubert. Des modifications et des ajouts ont été apportés pour améliorer l'extraction, le nettoyage, la construction, et la visualisation des données. Suite à mes modifications et mes ajouts, le script se divise en plusieurs parties ((Pour voir le script R, veuillez vous référer à l'annexe : [Script_R_reseau_co-auteur](#reseau-co-auteur)) :

La première partie correspond à l'extraction et au nettoyage des données. Les données brutes ont été organisées et nettoyées pour être adaptées à la construction du réseau. Les années de publication ont été extraites et les identifiants des auteurs ont été nettoyés. Les auteurs ont été associés aux structures pertinentes, et les incohérences de noms ou d'identifiants HAL ont été corrigées. Des identifiants fictifs ont été créés pour les auteurs sans identifiant.

La deuxième partie concerne la construction et la visualisation du graphe. À partir des données nettoyées, des matrices de contingence ont été créées. Ces matrices ont été converties en listes de nœuds (auteurs) et en listes d'arêtes (collaborations). Les nœuds ont été colorés et groupés en fonction des années de publication, et des légendes ont été ajoutées pour faciliter la compréhension du graphe. Des descriptions supplémentaires ont été intégrées pour enrichir les légendes, et la visualisation finale du réseau a été réalisée via visnetwork.

Il est possible de voir via l'exemple suivant (obtenu avec le code de l'annexe : [Script_R_reseau_co-auteur](#reseau-co-auteur)) qu'un réseau de co-auteurs permet une visualisation claire des collaborations et des périodes d'activité des différents auteurs.

Parler du quarto avec la construction d'un réseau et la création d'un réseau sur un soous groupe puis parler du premier sbm réalisé;

### Conclusion des Résultats

Exemple via chatGPT

Les méthodes développées au cours de ce stage ont permis d'améliorer significativement les performances des algorithmes existants. L'optimisation de l'algorithme X a réduit le temps de traitement de 40%, tandis que les ajustements apportés au modèle Y ont augmenté la précision de 5%. Ces résultats démontrent l'efficacité des approches choisies pour répondre aux limitations des méthodes précédentes.

## DISCUSSION

Au cours de cette partie sera aborder plusieurs sujets. Les limitations actuelles du package ManagHAL, et les possibles améliorations du package seront discutées. Les résultats obtenus seront replacés dans un contexte scientifique plus large. Enfin les potentiels utilisations du package ManagHAL seront discutées.

Premièrement, il est important de parler des limitations rencontrées au cours de ce projet. Un des défis a été l'intégration de l'API HAL. Malgré mes efforts, l'API HAL s'est révélée complexe et souvent peu intuitive. En effet, il existe des centaines de champs pour récupérer des données sur l'API HAL. Cela m'a freiné dans la compréhension de l'API et dans la modification des fonctions utilisant cette dernière. Les erreurs rencontrées étaient souvent dues à une mauvaise récupération des données, ou à une confusion de ma part par rapport au formatage de ces dernières. J'ai donc eu besoin de nombreuse heures de recherches, de débogage et de tests pour obtenir des résultats attendus.

De plus, bien que le code développé ait montré une certaine efficacité, il reste des marges d'amélioration. Par exemple, certaines parties du code pourraient être optimisées pour améliorer les performances des fonctions et réduire le temps de traitement. Par exemple, remplacer les boucles `for` par `apply`, qui est optimisée et plus rapide. De même pour la création de réseau et l'utilisation de modélisation en bloc stochastique, il est encore possible d'améliorer le temps de traitement en recherchant et modifiant les options des fonctions utilisé ou en créant des sous-groupes comme présenté dans les résultats.

Une des ambitions de ce projet est de s'inscrire dans une dynamique de science ouverte. Le mémoire écrit et produit avec Quarto illustre cette démarche en rendant le mémoire et les résultats accessibles et reproductibles sur un répertoire github (lien vers mon répertoire) .

S'il était possible de lier les résultats obtenus à des résultats obtenus avec d'autres bases de données telle qu WebOfScience, la valeur scientifique des recherches effectué via le package managHAL pourrait être considérablement enrichi. Par exemple, intégrer des données provenant d'autres sources pourrait permettre de croiser les informations et d'obtenir des analyses plus robustes.

Avant la fin du stage, je prévois de travailler sur la documentation et les tests. L'objectif est d'améliorer la documentation du code pour faciliter son utilisation par d'autres développeurs, d'améliorer la documentation des fonctions pour faciliter leurs utilisations par d'autres personnes et rajouter des tests pour garantir la bonne fonctionnalité du package. Je prévois aussi d'ajouter de nouvelles fonctionnalités. Dans un premier temps, je souhaite codé les fonctions et sous-fonctions permettant de construire un réseau de co-autheur plutot que d'avoir un simple script R. Puis je souhaite codé les fonctions permettant d'obtenir un modèle en bloc stochastique via le package 'sbm'. Cela permettrai de simplifier l'utilisation du package et de le rendre plus générique. Dans un second temps, je prévois de nettoyer le package entièrement et de vérifier qu'il est bien aux normes pour pour qu'il puisse être publié à la fin de mon stage.

En ce qui concerne les utilisations potentielles du package développé, il y a des parallèles intéressants à explorer avec d'autres domaines scientifiques, comme la biologie. Par exemple, les réseaux de gènes en bioinformatique pourraient bénéficier des algorithmes et des structures de données développés dans ce projet. Les techniques de traitement et d'analyse de données utilisées ici pourraient être adaptées pour analyser les interactions entre gènes, ouvrant ainsi de nouvelles perspectives de recherche interdisciplinaire.

Conclusion En conclusion, ce stage a permis de réaliser des avancées significatives malgré les défis rencontrés. La discussion autour des limitations, des perspectives d'amélioration et des potentiels liens avec d'autres domaines scientifiques démontre une maturité scientifique et une capacité à réfléchir de manière critique et constructive. Les prochaines étapes incluront des optimisations et des développements supplémentaires pour renforcer l'impact et la portée du travail réalisé, tout en restant aligné avec les principes de la science ouverte.

Cette démarche, combinée à une réflexion continue sur les objectifs et les résultats, permettra non seulement d'améliorer la qualité du travail actuel, mais aussi de poser les bases pour des recherches futures prometteuses.

## ANNEXES

### load_team_table_url {#load-team-table-url .Sections}

```{r load_team_table_url example, eval=FALSE, tidy.opts=list(width.cutoff=50)}
load_team_table_url <- function(filter_id = TRUE,
                                date_cols,
                                url) {
  idhal <- NULL
  tmp_file <- paste0(tempfile(), ".xlsx")
  download.file(
    url = url,
    destfile = tmp_file,
    mode = "wb"
  )
  nbcol <- ncol(readxl::read_xlsx(tmp_file,
                                  sheet = "Membres"))
  col_types <- rep("text",nbcol)
  col_types[date_cols] <- 'date'
  
  
  table <- readxl::read_xlsx(tmp_file,
                             sheet = "Membres",
                             col_types = col_types
  ) %>%
    dplyr::rename_with(
      clean_names
    ) %>%
    dplyr::mutate_if(function(x) {
      "POSIXt" %in% class(x)
    }, ~ format(.x, "%d/%m/%Y"))
  
  
  if (filter_id) {
    table <- suppressWarnings(table %>%
                                dplyr::filter(!is.na(as.numeric(idhal))) %>%
                                dplyr::mutate(idhal = as.numeric(idhal)))
  }
  
  # Test if there is a column named "id"
  # Possibility to add mandatory columns in the future
  # Just add "OR"
  if ( !("idhal" %in% names(table)) | !("nom" %in% names(table)) | !("prenom" %in% names(table)) ) { stop("one of the following columns are missing : 'idhal' ; 'nom' ; 'prenom' ") }
  
  return(table)
}
```

### load_team_table_csv {#load-team-table-csv .Sections}

```{r load_team_table_csv example, eval=FALSE, tidy.opts=list(width.cutoff=50)}
load_team_table_csv <- function(filter_id = TRUE,
                                date_cols,
                                filepath){
  idhal <- NULL
  
  # read the csv table  TODO : Add message error to guide user
  # if sep is different than ";" 
  # and if header = false
  nbcol <- ncol(read.csv(filepath, header = TRUE, sep = ";", encoding = "UTF-8"))
  
  # define column types (dates or text if not dates)
  col_types <- rep("character", nbcol)
  col_types[date_cols] <- 'character'
  
  table <- read.csv(filepath,
                    header = TRUE,
                    sep = ";",
                    encoding = "UTF-8",
                    colClasses = col_types
  ) %>%
    dplyr::rename_with(  # clean the columns names 
      clean_names
    ) %>%
    dplyr::mutate_if(function(x) {  # Convert specified column to dates
      "POSIXt" %in% class(x)
    }, ~ format(.x, "%d/%m/%Y"))
  
  # Filter out rows without IDHAL
  if (filter_id) {
    idhal <- table$idhal
    table <- table[which(!is.na(idhal)),]
    table$idhal <- as.numeric(idhal)
    table$idhal[table$idhal == 0] <- NA
    table <- table[which(!is.na(idhal)),]
  }
  
  # Test if there is a column named "id"
  # Possibility to add mandatory columns in the future
  # Just add "OR"
  if ( !("idhal" %in% names(table)) | !("nom" %in% names(table)) | !("prenom" %in% names(table)) ) { stop("one of the following columns are missing : 'idhal' ; 'nom' ; 'prenom' ") }
  
  return(table)
}
```

### load_team_table {#load-team-table .Sections}

```{r load_team_table example, eval=FALSE, tidy.opts=list(width.cutoff=50)}
load_team_table <- function(filter_id = TRUE,
                            date_cols = c(7,8),
                            filepath_or_url) {
  
  # If the parameter is a URL
  if (grepl("^https?://", filepath_or_url)) {
    
    # Call the load_team_table_url function with the URL
    return(load_team_table_url(filter_id = filter_id, date_cols = date_cols, url = filepath_or_url))
  } else {
    
    # Otherwise, it's a local file path; call the load_team_table_csv function with the local file path
    return(load_team_table_csv(filter_id = filter_id, date_cols = date_cols, filepath = filepath_or_url))
  }
}
```

### Exemple d'une documentation classique avec Roxygen2 {#doc-Roxygen2 .Sections}

```{r doc example, eval=FALSE, tidy.opts=list(width.cutoff=50)}
#' load_team_table_csv
#'
#' This function loads a team table from a CSV file, allowing optional filtering 
#' based on the presence of an IDHAL an specifying which columns contain dates.
#'
#' @param filter_id erase person without IDHAL (default = TRUE)
#' @param date_cols positions of dates columns, others will be read as text
#' @param filepath the file path of the CSV table
#'
#' @export
#'
#' @importFrom magrittr %>%
#' @importFrom utils read.csv
#'
#' @return a data.frame containing information from team
#'
#' @examples
#' 
#' \dontrun{
#' load_team_table_csv(filter_id = TRUE,
#'                     date_cols = c(7,8),
#'                     "C:/users/.../.../Classeur.csv")
#' }
```

### HAL_query {#HAL-query .Sections}

```{r hal query, tidy.opts=list(width.cutoff=50)}
HAL_query <- function(id,
                      date_min = NULL,
                      date_max = NULL,
                      format = c("csv", "bibtex", "json", "xml", "online"),
                      grouped = FALSE,
                      type_id = c("person_id", "struct_id"),
                      maxrows = 1000,
                      add_filters = list(),
                      add_exclusions = list(
                        status_i = 111,
                        instance_s = c("sfo", "dumas", "memsic", "hceres")
                      ),
                      add_outputs = character(0),
                      sorted_by = c("producedDate_tdate", "desc"),
                      thesis_strict = T) {
  
  # ----- DEFAULT PARAM -------------------------------------------------------
  online <- format[[1]] == "online"
  outputs <- c(
    "docid", "halId_s", "version_i", "docType_s", "citationFull_s",
    "citationRef_s", "publicationDate_tdate", "authFullNamePersonIDIDHal_fs", add_outputs
  )
  filters <- list(
    docType_s = c("COMM", "ART", "OUV", "COUV", "DOUV", "POSTER", "SOFTWARE", "THESE", "HDR")
  )
  filters[names(add_filters)] <- add_filters
  
  exclusions <- list()
  exclusions[names(add_exclusions)] <- add_exclusions
  # ---------------------------------------------------------------------------
  
  # Url start
  if (online) {
    start_query <- "https://hal.archives-ouvertes.fr/search/index/?"
  } else {
    start_query <- paste0(
      "https://api.archives-ouvertes.fr/search/hal/", # API HAL
      "?omitHeader=true",                             # header ommited
      "&wt=", format[[1]], "&"                        # csv, bibtex, ...
    )
  }
  if (grouped) {
    date_query <- query_date_grouped_parsing(
      id = id,
      type_id = type_id,
      date_min = date_min,
      date_max = date_max,
      online = online
    )
  } else {
    date_query <- query_date_ungrouped_parsing(
      id = id,
      type_id = type_id,
      date_min = date_min,
      date_max = date_max,
      online = online
    )
  }
  
  filter_query <- query_filter_parsing(
    filters = filters, exclusions = exclusions,
    online = online
  )
  if (thesis_strict & !online) {
    add <- "&fq=NOT+(docType_s:(THESE+OR+HDR)+AND+submitType_s:(notice+OR+annex))"
  } else {
    add <- ""
  }
  
  sorting_query <- query_sort_parsing(sorted_by = sorted_by)
  output_query <- query_output_parsing(
    outputs = outputs, n_row = maxrows,
    online = online
  )
  obj <- list(
    urls = paste0(
      start_query,
      date_query,
      filter_query,
      add,
      sorting_query,
      output_query
    ) %>%
      utils::URLencode(),
    description = list(
      idhal = id,
      date_min = date_min,
      date_max = date_max,
      outputs = outputs,
      format = format[[1]],
      grouped = grouped,
      maxrows = maxrows,
      filters = filters,
      exclusions = exclusions,
      outputs = outputs,
      sorted_by = sorted_by,
      thesis_strict = thesis_strict
    )
  )
  class(obj) <- "halUrl"
  return(obj)
}
```

### Script_R_reseau_co-auteur {#reseau-co-auteur .Sections}

```{r script reseau co auteur, eval=FALSE, tidy.opts=list(width.cutoff=50)}

#### ------------------- Creation of a co-authors graph ------------------- ####

#### --------- First part : Data extraction and cleaning

publication_id_struc <- publication_id_struc %>%
  mutate(Publication_Year = substr(publicationDate_tdate, 1, 4))

for (i in seq_along(publication_id_struc$structHasAlphaAuthIdHalPersonid_fs)) {
  publication_id_struc$structHasAlphaAuthIdHalPersonid_fs[i] <- gsub("\\\\", "", publication_id_struc$structHasAlphaAuthIdHalPersonid_fs[i])
}

# extraction of info on the authors
data_graph_authors <- publication_id_struc %>%
  select(halId_s, Publication_Year, authFullNamePersonIDIDHal_fs) %>%
  separate_rows(authFullNamePersonIDIDHal_fs, sep = ",") %>%
  separate(authFullNamePersonIDIDHal_fs, into = c("Full_Name", "Person_ID", "idhal"), sep = "_FacetSep_", fill = "right") %>%
  mutate(across(c(Full_Name, Person_ID, idhal), as.character)) %>%
  mutate(Person_ID = ifelse(Person_ID == "0", NA, Person_ID))

# extraction of info on which authors belongs to which structure at the times of the publications
data_graph_author_struc <- publication_id_struc %>%
  select(structHasAlphaAuthIdHalPersonid_fs) %>%
  separate_rows(structHasAlphaAuthIdHalPersonid_fs, sep = "_AlphaSep_") %>%
  separate(structHasAlphaAuthIdHalPersonid_fs, into = c("id_name_struc", "id_full_name"), sep = "_JoinSep_") %>%
  separate(id_full_name, into = c("idhal_s", "Person_ID", "Full_Name"), sep = "_FacetSep_", fill = "right") %>%
  separate(id_name_struc, into = c("id_struc" , "name_struc"), sep = "_FacetSep_", fill = "right")  %>%
  separate(Full_Name, into = c("Full_Name", "First_Letter"), sep = "," , fill = "right") %>%
  mutate(Person_ID = ifelse(Person_ID == "0", NA, Person_ID))
## data_graph_author_struc <- find_inconsistent_names_and_ids(data_graph_author_struc)
data_graph_authors <- find_inconsistent_names_and_ids(data_graph_authors)
data_graph_authors <- merge(data_graph_authors, data_graph_author_struc[, c("Person_ID", "id_struc")], by = "Person_ID", all.x = TRUE)


# Creation of false ids only for authors without Person_ID
# addind a mark to know which one were created and which were not
data_graph_authors$False_id_mark <- !is.na(data_graph_authors$Person_ID)
unique_authors_without_id_vec <- unique(data_graph_authors$Full_Name[which(data_graph_authors$False_id_mark== FALSE)]) ##%>%
  ##distinct(Full_Name, .keep_all = TRUE) ##%>%
  ##filter(is.na(Person_ID))

# to create my own ids
creation_id <- function(unique_authors_no_id_name_column) {
  ids <- character(length(unique_authors_no_id_name_column))  
  count <- 0
  for (i in 1:length(unique_authors_no_id_name_column)) {
    count <- count + 1
    ids[i] <- count  
  }
  return(ids)
}
unique_authors_without_id <- data.frame(Full_Name=unique_authors_without_id_vec)
unique_authors_without_id$False_Id = creation_id(unique_authors_without_id$Full_Name)

# delete lines where False_Id equals Person_ID
# i lose info here but one person out of a thousands seems ok (to be tested)
unique_authors_without_id <- unique_authors_without_id %>%
  filter(!False_Id %in% data_graph_authors$Person_ID)
  
         
# give a false id to all authors without person id
data_graph_authors <- data_graph_authors %>%
  left_join(unique_authors_without_id %>% select(Full_Name, False_Id), by = "Full_Name") %>%
  mutate(Person_ID = ifelse(is.na(Person_ID), False_Id, Person_ID)) %>%
  select(-False_Id, -idhal)


#### --------- Second part : Constructing and visualizing networks 

# Creating the "edge list" and the "node list"
## To do so we first create the contingence matrix, and then we applied the melt function
contingence_table = data_graph_authors %>%
  count(as.numeric(Person_ID), halId_s) %>%
  dplyr::select(- n) %>% table %>% as.matrix
contingence_matrix = contingence_table %*% t(contingence_table)
contingence_matrix[lower.tri(contingence_matrix)] = 1000 # We set 1000 in order to applied a filter later on (in order to avoid duplication with the melt function)
melt_matrix = melt(contingence_matrix);   colnames(melt_matrix) = c("from", "to", "value")
edge_list = melt_matrix %>% filter(from != to & !value %in% c(0, 1000) ) %>% rename(width = value)
node_list = melt_matrix %>% filter(from == to) %>% dplyr::select(- to) %>% rename(id = from) %>% mutate(label = id)

# Creating a group variable in the node_list in order to set differents colors corresponding with the different years
data_graph_authors$Publication_Year <- as.numeric(as.character(data_graph_authors$Publication_Year))
df_group = data_graph_authors %>% 
  summarise(group = as.character (floor (mean (Publication_Year))), .by = Person_ID) %>% rename (id = Person_ID)
node_list$id <- as.character(node_list$id)
node_list = node_list %>%
  left_join (df_group, by = "id") 

# Defining nodes and edges that will serve to build a legend for edges and nodes sizes (number of publications), and colors (years)
range_year = range (data_graph_authors$Publication_Year)
node_size = data.frame(id = c("A", "B", "C"), value = c(1, 6, 15), label = c("A", "B", "C"), group = "Number of publications")
edge_size = data.frame(from = c("A", "B", "C"), to = c("B", "C", "A"), width = c(1, 6, 15), label = c("1", "6", "15"))
node_colors = data.frame (id = as.character (range_year), value = 1, label = as.character (range_year), group = as.character (range_year))
edge_colors = data.frame (from = as.character (range_year), to = as.character (range_year[c(2, 1)]), width  = 1, label = "")
node_list = rbind (node_list, node_size, node_colors)
edge_list = rbind (edge_list %>% mutate (label = ""), edge_size, edge_colors)
edge_list = edge_list %>% mutate(size = 20)

# Defining colors for Publication year
df_col = data.frame (
  group = as.character (c (range_year[1] : range_year[2])),
  color = colorRampPalette (colors = c ("#35B779FF", "#FDE725FF")) (range_year[2] - range_year[1] + 1)
)
node_list = node_list %>%
  left_join (df_col, by = "group")

# Legend nodes 
# Defining colors for Publication year
df_col_Legend = data.frame (
  label = as.character (c (range_year[1] : range_year[2])),
  group = as.character (c (range_year[1] : range_year[2])),
  color = colorRampPalette (colors = c ("#35B779FF", "#FDE725FF")) (range_year[2] - range_year[1] + 1))

# legend additional information table
node_info <- data.frame(
  id = c("A", "B", "C"),
  info = c("Additional info for node A", "Additional info for node B", "Additional info for node C")
)

# Join node_info with node_list to add the title column to the legend nodes
node_list <- node_list %>%
  left_join(node_info, by = "id") %>%
  mutate(title = ifelse(!is.na(info), info, "")) %>%
  select(-info) # remove the info column after using it to create the title

# Convert idhal to character type
cleaned_table <- cleaned_table %>%
  mutate(idhal = as.character(idhal))

 tmp2 <- data_graph_authors[data_graph_authors$Person_ID %in% node_list$id,]
 node_info2 <- tmp2 %>%
  select(Full_Name, Person_ID) %>%
  rename(id = Person_ID) 
 
 # Constructing node_info3
 tmp3 <- cleaned_table[cleaned_table$idhal %in% node_list$id,]
 node_info3 <- tmp3 %>%
  select(idhal, nom, prenom, unite, equipe) %>%
  rename(id = idhal) 

# Joining node_info2 with node_list to add the title column
node_list <- node_list %>%
  left_join(node_info2, by = "id") %>%
  mutate(title_info = ifelse(!is.na(Full_Name), paste("Name:", Full_Name,",", "numeric idHal:", id), NA),
         title = ifelse(title == "", title_info, title))

# Joining node_info3 with node_list to add the title column
node_list <- node_list %>%
  left_join(node_info3, by = "id") %>%
  mutate(title_info = ifelse(!is.na(nom), paste("Name:", paste(nom, prenom), ",", "Team:", equipe, ",", "Unit:", unite, "," , "numeric idHal:", id), title_info)) %>%
  mutate(title = if_else(!is.na(nom), title_info, title)) %>%
  select(-title_info)

node_list <- node_list %>% distinct(node_list$id, .keep_all = TRUE)

# Network visualization
visNetwork(node_list, edge_list, width = "100%") %>%
  visIgraphLayout() %>%
  visGroups(groupname = "Number of publications", color = "lightblue", font = list(size = 22)) %>%
  visLegend(addNodes = df_col_Legend, main = "Graph of co-authors", useGroups = FALSE) %>%
  visEdges(font = list(size = 22)) %>%
  visOptions(selectedBy = "equipe") %>% 
  visExport( type = "jpeg", name = "Full_Network_Co-authors_Example_managHAL", label = "Export as jpeg")

```

### exemple_reseau_unipartite_bipartite {#reseau-unipart-bipart .Sections}

```{r exemple de reseau, eval = FALSE, tidy.opts=list(width.cutoff=50)}
# Création d'un réseau unipartite
set.seed(42) # Pour la reproductibilité 
unipartite <- erdos.renyi.game(10, 0.3)
# unipartite_graph <- plot(unipartite, main = "Réseau Unipartite", width = 800, height = 600)
# print(unipartite_graph)
matrice_adjacence_unipartite <- as.matrix(unipartite, "adjacency")

# Création d'un réseau bipartite
# Noms des nœuds
nodes_A <- c("A1", "A2", "A3")
nodes_B <- c("B1", "B2", "B3", "B4")
# Liste des arêtes
edges <- c("A1", "B1", "A1", "B2","A2", "B1", "A2", "B2", "A2", "B3", "A3", "B2", "A3", "B4")
# Création du graphe
bipartite <- graph(edges, directed = FALSE)
V(bipartite)$type <- bipartite_mapping(bipartite)$type
plot(bipartite, layout = layout_as_bipartite, vertex.color = c("skyblue", "salmon")[V(bipartite)$type + 1],
     vertex.label.color = "black", vertex.shape = "circle", vertex.size = 30, edge.width = 2, main = "Réseau Bipartite")
matrice_adjacence_bipartite <- as.matrix(bipartite, "adjacency")
```

### Tableau des résultats détaillés

Voici les tableaux des résultats détaillés pour les différentes méthodes et tests effectués.

## BIBLIOGRAPHIE
